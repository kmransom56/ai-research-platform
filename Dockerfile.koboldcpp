FROM nvidia/cuda:12.2-devel-ubuntu22.04 as builder

LABEL maintainer="AI Research Platform Team"
LABEL description="KoboldCpp for Creative Writing and Roleplay"

# Install build dependencies
RUN apt-get update && apt-get install -y \
    git \
    build-essential \
    cmake \
    python3 \
    python3-pip \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Clone and build KoboldCpp
WORKDIR /build
RUN git clone https://github.com/LostRuins/koboldcpp.git
WORKDIR /build/koboldcpp

# Build with CUDA support
RUN make LLAMA_CUBLAS=1

# Production image
FROM nvidia/cuda:12.2-runtime-ubuntu22.04

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy built binary from builder stage
COPY --from=builder /build/koboldcpp/koboldcpp /usr/local/bin/
COPY --from=builder /build/koboldcpp/*.so /usr/local/lib/

# Set library path
ENV LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH

# Create app directory and models directory
WORKDIR /app
RUN mkdir -p models

# Create startup script
RUN cat > start_koboldcpp.sh << 'EOF'
#!/bin/bash
set -e

MODEL_PATH=${MODEL_PATH:-"/app/models/default.gguf"}
PORT=${PORT:-"5001"}
CONTEXT_SIZE=${CONTEXT_SIZE:-"8192"}
GPU_LAYERS=${GPU_LAYERS:-"32"}

echo "Starting KoboldCpp..."
echo "Model: $MODEL_PATH"
echo "Port: $PORT"
echo "Context Size: $CONTEXT_SIZE"
echo "GPU Layers: $GPU_LAYERS"

if [ ! -f "$MODEL_PATH" ]; then
    echo "Model file not found: $MODEL_PATH"
    echo "Available models:"
    ls -la /app/models/
    
    # Download a default small model if no model is available
    if [ ! -f "/app/models/default.gguf" ]; then
        echo "Downloading default model..."
        curl -L -o "/app/models/default.gguf" \
            "https://huggingface.co/microsoft/DialoGPT-small-GGUF/resolve/main/dialo-gpt-small.gguf"
    fi
    MODEL_PATH="/app/models/default.gguf"
fi

exec koboldcpp \
    --model "$MODEL_PATH" \
    --port "$PORT" \
    --host 0.0.0.0 \
    --gpulayers "$GPU_LAYERS" \
    --usecuda \
    --contextsize "$CONTEXT_SIZE" \
    --blasbatchsize 512 \
    --highpriority
EOF

RUN chmod +x start_koboldcpp.sh

# Create non-root user
RUN groupadd -r kobold && useradd -r -g kobold -d /app -s /bin/bash kobold
RUN chown -R kobold:kobold /app
USER kobold

# Health check
HEALTHCHECK --interval=60s --timeout=30s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:5001/api/v1/info || exit 1

# Expose port
EXPOSE 5001

# Environment variables
ENV MODEL_PATH=/app/models/default.gguf
ENV PORT=5001
ENV CONTEXT_SIZE=8192
ENV GPU_LAYERS=32

# Start KoboldCpp
CMD ["./start_koboldcpp.sh"]