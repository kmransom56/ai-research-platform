version: '3.8'

networks:
  ai-platform:
    name: chatcopilot_ai-platform
    driver: bridge

volumes:
  postgres-data:
  rabbitmq-data:
  qdrant-data:
  neo4j-data:
  ollama-data:
  openwebui-data:
  vscode-data:
  grafana-data:
  # AI Stack volumes
  vllm-cache:
  oobabooga-models:
  koboldcpp-models:

services:
  # Infrastructure Services
  postgres:
    image: postgres:15
    container_name: ai-platform-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_DB=chatcopilot
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=chatcopilot-password
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - ai-platform

  rabbitmq:
    image: rabbitmq:3-management
    container_name: ai-platform-rabbitmq
    restart: unless-stopped
    environment:
      - RABBITMQ_DEFAULT_USER=chatcopilot
      - RABBITMQ_DEFAULT_PASS=chatcopilot-password
    ports:
      - "5672:5672"
      - "15672:15672"
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq
    networks:
      - ai-platform

  qdrant:
    image: qdrant/qdrant:latest
    container_name: ai-platform-qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant-data:/qdrant/storage
    networks:
      - ai-platform

  neo4j:
    image: neo4j:5
    container_name: ai-platform-neo4j
    restart: unless-stopped
    environment:
      - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD}
      - NEO4J_PLUGINS=["apoc"]
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - neo4j-data:/data
    networks:
      - ai-platform

  # Existing AI Services  
  ollama:
    image: ollama/ollama:latest
    container_name: ai-platform-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - ai-platform

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ai-platform-openwebui
    restart: unless-stopped
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    ports:
      - "11880:8080"
    volumes:
      - openwebui-data:/app/backend/data
    networks:
      - ai-platform
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # Advanced AI Stack Services
  
  # AI Stack API Gateway
  ai-gateway:
    build:
      context: .
      dockerfile: Dockerfile.ai-gateway
    container_name: ai-platform-ai-gateway
    restart: unless-stopped
    ports:
      - "9000:9000"
    environment:
      - REASONING_MODEL_URL=http://host.docker.internal:8000
      - GENERAL_MODEL_URL=http://host.docker.internal:8001
      - CODING_MODEL_URL=http://host.docker.internal:8002
      - CREATIVE_MODEL_URL=http://host.docker.internal:5001
      - ADVANCED_MODEL_URL=http://host.docker.internal:5000
    volumes:
      - ./logs:/app/logs
    networks:
      - ai-platform
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - postgres
      - neo4j

  # Oobabooga Text Generation WebUI (Tesla K80 optimized)
  oobabooga:
    image: ghcr.io/oobabooga/text-generation-webui:latest
    container_name: ai-platform-oobabooga
    restart: unless-stopped
    ports:
      - "7860:7860"  # Web UI
      - "5000:5000"  # API
    environment:
      - CLI_ARGS=--listen --listen-port 7860 --api --api-port 5000 --loader llamacpp --gpu-memory 10 --n-gpu-layers 20
      - CUDA_VISIBLE_DEVICES=0
      - TORCH_CUDA_ARCH_LIST=3.7  # Tesla K80 compute capability
    volumes:
      - oobabooga-models:/app/models
      - ./python/ai-stack/oobabooga-config:/app/characters
      - ./python/ai-stack/tesla_k80_config.py:/app/tesla_k80_config.py
    networks:
      - ai-platform
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]
        limits:
          memory: 16G

  # KoboldCpp (Tesla K80 optimized)
  koboldcpp:
    build:
      context: .
      dockerfile: Dockerfile.koboldcpp
    container_name: ai-platform-koboldcpp
    restart: unless-stopped
    ports:
      - "5001:5001"
    environment:
      - CUDA_VISIBLE_DEVICES=1
      - GPU_LAYERS=32
      - CONTEXT_SIZE=4096  # Conservative for Tesla K80 memory
      - MODEL_PATH=/app/models/tesla-k80-optimized.gguf
    volumes:
      - koboldcpp-models:/app/models
      - ./python/ai-stack/tesla_k80_config.py:/app/tesla_k80_config.py
    networks:
      - ai-platform
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["1"]
              capabilities: [gpu]
        limits:
          memory: 16G

  # AI Stack Monitor
  ai-monitor:
    build:
      context: .
      dockerfile: Dockerfile.ai-monitor
    container_name: ai-platform-ai-monitor
    restart: unless-stopped
    ports:
      - "8090:8090"
    environment:
      - GATEWAY_URL=http://ai-gateway:9000
    volumes:
      - ./logs:/app/logs
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - ai-platform
    depends_on:
      - ai-gateway

  # Development Tools
  vscode:
    image: codercom/code-server:latest
    container_name: ai-platform-vscode
    restart: unless-stopped
    environment:
      - PASSWORD=chatcopilot
    ports:
      - "57081:8080"
    volumes:
      - .:/home/coder/workspace
      - vscode-data:/home/coder/.local/share/code-server
    networks:
      - ai-platform

  # Monitoring
  grafana:
    image: grafana/grafana:latest
    container_name: ai-platform-grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    ports:
      - "11002:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
    networks:
      - ai-platform

  prometheus:
    image: prom/prometheus:latest
    container_name: ai-platform-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    networks:
      - ai-platform
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'