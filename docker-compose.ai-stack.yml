# Docker Compose for Advanced AI Stack
# vLLM + Oobabooga + KoboldCpp + API Gateway

networks:
  ai-platform:
    name: chatcopilot_ai-platform
    driver: bridge

volumes:
  postgres-data:
  rabbitmq-data:
  qdrant-data:
  neo4j-data:
  ollama-data:
  openwebui-data:
  vscode-data:
  grafana-data:
  # AI Stack volumes
  vllm-cache:
  oobabooga-models:
  koboldcpp-models:

services:
  # Infrastructure Services
  postgres:
    image: postgres:15
    container_name: ai-platform-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_DB=chatcopilot
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=chatcopilot-password
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - ai-platform

  rabbitmq:
    image: rabbitmq:3-management
    container_name: ai-platform-rabbitmq
    restart: unless-stopped
    environment:
      - RABBITMQ_DEFAULT_USER=chatcopilot
      - RABBITMQ_DEFAULT_PASS=chatcopilot-password
    ports:
      - "5672:5672"
      - "15672:15672"
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq
    networks:
      - ai-platform

  qdrant:
    image: qdrant/qdrant:latest
    container_name: ai-platform-qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant-data:/qdrant/storage
    networks:
      - ai-platform

  neo4j:
    image: neo4j:5
    container_name: ai-platform-neo4j
    restart: unless-stopped
    environment:
      - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD}
      - NEO4J_PLUGINS=["apoc"]
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - neo4j-data:/data
    networks:
      - ai-platform

  # Existing AI Services  
  ollama:
    image: ollama/ollama:latest
    container_name: ai-platform-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - ai-platform

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ai-platform-openwebui
    restart: unless-stopped
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    ports:
      - "11880:8080"
    volumes:
      - openwebui-data:/app/backend/data
    networks:
      - ai-platform
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # Advanced AI Stack Services
  
  # vLLM Services (High-Performance Inference)
  
  # vLLM DeepSeek R1 (Reasoning)
  vllm-reasoning:
    image: vllm/vllm-openai:latest
    container_name: ai-platform-vllm-reasoning
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_TOKEN=${HUGGINGFACE_TOKEN}
      - VLLM_ATTENTION_BACKEND=FLASHINFER
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: >
      --model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
      --port 8000
      --host 0.0.0.0
      --gpu-memory-utilization 0.8
      --max-model-len 8192
      --tensor-parallel-size 1
      --trust-remote-code
    networks:
      - ai-platform
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
        limits:
          memory: 24G

  # vLLM Mistral Small (General Purpose)
  vllm-general:
    image: vllm/vllm-openai:latest
    container_name: ai-platform-vllm-general
    restart: unless-stopped
    ports:
      - "8001:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=1
      - HF_TOKEN=${HUGGINGFACE_TOKEN}
      - VLLM_ATTENTION_BACKEND=FLASHINFER
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: >
      --model mistralai/Mistral-7B-Instruct-v0.3
      --port 8000
      --host 0.0.0.0
      --gpu-memory-utilization 0.8
      --max-model-len 8192
      --tensor-parallel-size 1
      --trust-remote-code
    networks:
      - ai-platform
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
        limits:
          memory: 24G

  # vLLM DeepSeek Coder (Code Generation)
  vllm-coding:
    image: vllm/vllm-openai:latest
    container_name: ai-platform-vllm-coding
    restart: unless-stopped
    ports:
      - "8002:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=2
      - HF_TOKEN=${HUGGINGFACE_TOKEN}
      - VLLM_ATTENTION_BACKEND=FLASHINFER
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: >
      --model deepseek-ai/deepseek-coder-6.7b-instruct
      --port 8000
      --host 0.0.0.0
      --gpu-memory-utilization 0.8
      --max-model-len 8192
      --tensor-parallel-size 1
      --trust-remote-code
    networks:
      - ai-platform
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
        limits:
          memory: 24G
  
  # AI Stack API Gateway
  ai-gateway:
    build:
      context: .
      dockerfile: Dockerfile.ai-gateway
    container_name: ai-platform-ai-gateway
    restart: unless-stopped
    ports:
      - "9000:9000"
    environment:
      - REASONING_MODEL_URL=http://vllm-reasoning:8000
      - GENERAL_MODEL_URL=http://vllm-general:8000
      - CODING_MODEL_URL=http://vllm-coding:8000
      - CREATIVE_MODEL_URL=http://koboldcpp:5001
      - ADVANCED_MODEL_URL=http://oobabooga:5000
      - HF_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - ./logs:/app/logs
    networks:
      - ai-platform
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - postgres
      - neo4j
      - vllm-reasoning
      - vllm-general
      - vllm-coding
      - koboldcpp
      - oobabooga

  # Oobabooga Text Generation WebUI (RTX 3060 optimized)
  oobabooga:
    image: atinoda/text-generation-webui:default
    container_name: ai-platform-oobabooga
    restart: unless-stopped
    ports:
      - "7860:7860"  # Web UI
      - "5001:5000"  # API (changed to avoid conflict)
    environment:
      - CLI_ARGS=--listen --listen-port 7860 --api --api-port 5000 --loader transformers --gpu-memory 8 --n-gpu-layers 35
      - CUDA_VISIBLE_DEVICES=1
      - TORCH_CUDA_ARCH_LIST=8.6  # RTX 3060 compute capability
      - PUID=0
      - PGID=0
    volumes:
      - oobabooga-models:/app/models
      - ./tmp/oobabooga-cache:/app/user_data/cache
      - ./tmp/oobabooga-logs:/app/logs
    user: "0:0"
    networks:
      - ai-platform
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]
        limits:
          memory: 16G

  # KoboldCpp (Tesla K80 optimized)
  koboldcpp:
    build:
      context: .
      dockerfile: Dockerfile.koboldcpp
    container_name: ai-platform-koboldcpp
    restart: unless-stopped
    ports:
      - "5001:5001"
    environment:
      - CUDA_VISIBLE_DEVICES=1
      - GPU_LAYERS=32
      - CONTEXT_SIZE=4096  # Conservative for Tesla K80 memory
      - MODEL_PATH=/app/models/default.gguf
      - PUID=1000
      - PGID=1000
    volumes:
      - koboldcpp-models:/app/models
      - ./python/ai-stack/tesla_k80_config.py:/app/tesla_k80_config.py
    networks:
      - ai-platform
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["1"]
              capabilities: [gpu]
        limits:
          memory: 16G

  # AI Stack Monitor
  ai-monitor:
    build:
      context: .
      dockerfile: Dockerfile.ai-monitor
    container_name: ai-platform-ai-monitor
    restart: unless-stopped
    ports:
      - "8090:8090"
    environment:
      - GATEWAY_URL=http://ai-gateway:9000
    volumes:
      - ./logs:/app/logs
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - ai-platform
    depends_on:
      - ai-gateway

  # Development Tools
  vscode:
    image: codercom/code-server:latest
    container_name: ai-platform-vscode
    restart: unless-stopped
    environment:
      - PASSWORD=chatcopilot
    ports:
      - "57081:8080"
    volumes:
      - .:/home/coder/workspace
      - vscode-data:/home/coder/.local/share/code-server
    networks:
      - ai-platform

  # Monitoring
  grafana:
    image: grafana/grafana:latest
    container_name: ai-platform-grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    ports:
      - "11002:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
    networks:
      - ai-platform

  prometheus:
    image: prom/prometheus:latest
    container_name: ai-platform-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    networks:
      - ai-platform
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'