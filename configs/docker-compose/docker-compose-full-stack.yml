# AI Research Platform - Complete Containerized Stack

networks:
  ai-platform:
    external: true
    name: chat-copilot_ai-platform

volumes:
  qdrant_data:
    driver: local
  rabbitmq_data:
    driver: local
  postgres_data:
    driver: local
  portscanner_data:
    driver: local
  webhook_logs:
    driver: local
  platform_logs:
    driver: local
  openwebui_data:
    driver: local
  vscode_config:
    driver: local
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local
  neo4j_import:
    driver: local
  genai_embedding_model:
    driver: local

services:
  
  # ===========================================
  # CORE AI RESEARCH PLATFORM SERVICES
  # ===========================================
  
  # Chat Copilot Backend (.NET WebAPI)
  chat-copilot-backend:
    build:
      context: ../../
      dockerfile: docker/webapi/Dockerfile
    container_name: ai-platform-chat-backend
    restart: unless-stopped
    environment:
      - ASPNETCORE_ENVIRONMENT=Production
      - ASPNETCORE_URLS=http://+:80
      # AZURE_OPENAI_KEY provided via Docker Secret
      - KernelMemory__Services__OpenAI__Endpoint=${AZURE_OPENAI_ENDPOINT}
      - SemanticKernel__Services__AzureOpenAI__ApiKey=${AZURE_OPENAI_KEY}
      - SemanticKernel__Services__AzureOpenAI__Endpoint=${AZURE_OPENAI_ENDPOINT}
      - Qdrant__Host=qdrant
      - Qdrant__Port=6333
      - RabbitMQ__Host=rabbitmq
      - PostgreSQL__Host=postgres
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - OLLAMA_BASE_URL=http://ollama:11434
      - OPENAI_API_BASE_URL=${OPENAI_API_BASE_URL:-https://api.openai.com/v1}
      - OPENAI_API_KEY_FILE=/run/secrets/OPENAI_API_KEY
      - AZURE_OPENAI_KEY_FILE=/run/secrets/AZURE_OPENAI_KEY
      - ANTHROPIC_API_KEY_FILE=/run/secrets/ANTHROPIC_API_KEY
      - GEMINI_API_KEY_FILE=/run/secrets/GEMINI_API_KEY
    # ports mapping removed â€“ backend reachable inside Docker network at 80
    volumes:
      - platform_logs:/app/logs
      - ../../webapi/appsettings.json:/app/appsettings.json:ro
      - ../../webapp/build:/app/wwwroot
      - ./docker/entrypoint-export-secrets.sh:/entrypoint-export-secrets.sh:ro
    secrets:
      - AZURE_OPENAI_KEY
      - OPENAI_API_KEY
      - ANTHROPIC_API_KEY
      - GEMINI_API_KEY
    networks:
      ai-platform:
        ipv4_address: 172.20.0.20
    depends_on:
      - qdrant
      - rabbitmq
      - postgres
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Chat Copilot Frontend (React)
  chat-copilot-frontend:
    build:
      context: ../../webapp
      dockerfile: Dockerfile
      args:
        - NODE_ENV=production
    container_name: ai-platform-chat-frontend
    restart: unless-stopped
    environment:
      - REACT_APP_BACKEND_URI=http://172.20.0.20:80
      - NODE_ENV=production
    ports:
      - "3000:3000"
    volumes:
      - platform_logs:/app/logs
    networks:
      ai-platform:
        ipv4_address: 172.20.0.21
    depends_on:
      - chat-copilot-backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3

  # AutoGen Studio (Python Multi-Agent System)
  autogen-studio:
    build:
      context: ../../python/autogen-studio
      dockerfile: Dockerfile
    container_name: ai-platform-autogen
    restart: unless-stopped
    environment:
      - PYTHONPATH=/app
      - AUTOGEN_CONFIG_LIST=/app/config/OAI_CONFIG_LIST.json
      - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT}
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - OLLAMA_BASE_URL=http://ollama:11434
      - OPENAI_API_BASE_URL=${OPENAI_API_BASE_URL:-https://api.openai.com/v1}
      - OPENAI_API_KEY_FILE=/run/secrets/OPENAI_API_KEY
      - AZURE_OPENAI_KEY_FILE=/run/secrets/AZURE_OPENAI_KEY
      - ANTHROPIC_API_KEY_FILE=/run/secrets/ANTHROPIC_API_KEY
      - GEMINI_API_KEY_FILE=/run/secrets/GEMINI_API_KEY
    entrypoint: ["/entrypoint-export-secrets.sh"]
    volumes:
      - ../../python/autogen-studio/config:/app/config:ro
      - platform_logs:/app/logs
      - ./docker/entrypoint-export-secrets.sh:/entrypoint-export-secrets.sh:ro
    secrets:
      - AZURE_OPENAI_KEY
      - OPENAI_API_KEY
      - ANTHROPIC_API_KEY
      - GEMINI_API_KEY
    ports:
      - "11001:8081"
    networks:
      ai-platform:
        ipv4_address: 172.20.0.22
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Webhook Server (Node.js GitHub Integration)
  webhook-server:
    build:
      context: ../../runtime-data
      dockerfile: Dockerfile.webhook
    container_name: ai-platform-webhook
    restart: unless-stopped
    environment:
      - NODE_ENV=production
      - WEBHOOK_PORT=11002
      - REPOSITORY_PATH=/app/repo
    entrypoint: ["/entrypoint-export-secrets.sh"]
    volumes:
      - webhook_logs:/app/logs
      - .:/app/repo:rw  # Mount repo for git operations
      - /var/run/docker.sock:/var/run/docker.sock  # Docker access for deployments
      - ./docker/entrypoint-export-secrets.sh:/entrypoint-export-secrets.sh:ro
    secrets:
      - GITHUB_WEBHOOK_SECRET
    ports:
      - "11002:11002"
    networks:
      ai-platform:
        ipv4_address: 172.20.0.23
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11002/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Magentic-One (Microsoft Multi-Agent)
  magentic-one:
    build:
      context: ../../python/services
      dockerfile: Dockerfile
    container_name: ai-platform-magentic
    restart: unless-stopped
    environment:
      - PYTHONPATH=/app
      - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT}
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - OLLAMA_BASE_URL=http://ollama:11434
      - OPENAI_API_BASE_URL=${OPENAI_API_BASE_URL:-https://api.openai.com/v1}
      - OPENAI_API_KEY_FILE=/run/secrets/OPENAI_API_KEY
      - AZURE_OPENAI_KEY_FILE=/run/secrets/AZURE_OPENAI_KEY
      - ANTHROPIC_API_KEY_FILE=/run/secrets/ANTHROPIC_API_KEY
      - GEMINI_API_KEY_FILE=/run/secrets/GEMINI_API_KEY
    entrypoint: ["/entrypoint-export-secrets.sh"]
    volumes:
      - platform_logs:/app/logs
      - ./docker/entrypoint-export-secrets.sh:/entrypoint-export-secrets.sh:ro
    secrets:
      - AZURE_OPENAI_KEY
      - OPENAI_API_KEY
      - ANTHROPIC_API_KEY
      - GEMINI_API_KEY
    ports:
      - "11003:11003"
    networks:
      ai-platform:
        ipv4_address: 172.20.0.24
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11003/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Port Scanner (Network Discovery Tool)
  port-scanner:
    build:
      context: /home/keith/port-scanner-material-ui
      dockerfile: Dockerfile
    container_name: ai-platform-portscanner
    restart: unless-stopped
    environment:
      - NODE_ENV=production
      - PORT=11010
    ports:
      - "11010:11010"
    volumes:
      - portscanner_data:/app/data
      - platform_logs:/app/logs
    networks:
      ai-platform:
        ipv4_address: 172.20.0.25
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11010"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===========================================
  # EXTERNAL AI SERVICES (Already Containerized)
  # ===========================================
  
  # OpenWebUI (Primary LLM Interface)
  openwebui:
    image: ghcr.io/open-webui/open-webui:cuda
    container_name: ai-platform-openwebui
    restart: unless-stopped
    environment:
      - WEBUI_SECRET_KEY=${OPENWEBUI_SECRET_KEY}
      - OPENAI_API_BASE_URL=${AZURE_OPENAI_ENDPOINT}
      - OPENAI_API_KEY=${AZURE_OPENAI_KEY}
    entrypoint: ["/entrypoint-export-secrets.sh"]
    volumes:
      - openwebui_data:/app/backend/data
      - ./docker/entrypoint-export-secrets.sh:/entrypoint-export-secrets.sh:ro
    secrets:
      - OPENWEBUI_SECRET_KEY
      - AZURE_OPENAI_KEY
    ports:
      - "11880:8080"
    networks:
      ai-platform:
        ipv4_address: 172.20.0.30
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Perplexica (AI Search with Internet Access)
  perplexica:
    image: itzcrazykns1337/perplexcia:main
    container_name: ai-platform-perplexica
    restart: unless-stopped
    environment:
      - SEARXNG_API_URL=http://searxng:8080
      - OPENAI_API_BASE_URL=${AZURE_OPENAI_ENDPOINT}
    entrypoint: ["/entrypoint-export-secrets.sh"]
    volumes:
      - ./docker/entrypoint-export-secrets.sh:/entrypoint-export-secrets.sh:ro
    secrets:
      - AZURE_OPENAI_KEY
    ports:
      - "11020:3000"
    networks:
      ai-platform:
        ipv4_address: 172.20.0.31
    depends_on:
      - searxng
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3

  # SearXNG (Privacy-Focused Search Engine)
  searxng:
    image: searxng/searxng:latest
    container_name: ai-platform-searxng
    restart: unless-stopped
    environment:
      - BASE_URL=https://ubuntuaicodeserver-1.tail5137b4.ts.net:8443/searxng/
      - INSTANCE_NAME=AI Research Platform Search
    ports:
      - "11021:8080"
    volumes:
      - ../../configs/searxng:/etc/searxng:ro
    networks:
      ai-platform:
        ipv4_address: 172.20.0.32
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3

  # VS Code Server (Web-based IDE)
  vscode-server:
    image: codercom/code-server:latest
    container_name: ai-platform-vscode
    restart: unless-stopped
    environment:
      - PASSWORD
      - SUDO_PASSWORD
    entrypoint: ["/entrypoint-export-secrets.sh"]
    volumes:
      - ../../:/home/coder/workspace
      - vscode_config:/home/coder/.config
      - ./docker/entrypoint-export-secrets.sh:/entrypoint-export-secrets.sh:ro
    secrets:
      - VSCODE_PASSWORD
    ports:
      - "57081:8080"
    networks:
      ai-platform:
        ipv4_address: 172.20.0.33
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===========================================
  # DATABASE & INFRASTRUCTURE SERVICES
  # ===========================================
  
  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: ai-platform-qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      ai-platform:
        ipv4_address: 172.20.0.40
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # RabbitMQ Message Queue
  rabbitmq:
    image: rabbitmq:3-management
    container_name: ai-platform-rabbitmq
    restart: unless-stopped
    environment:
      - RABBITMQ_DEFAULT_USER
      - RABBITMQ_DEFAULT_PASS
    entrypoint: ["/entrypoint-export-secrets.sh"]
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
      - ./docker/entrypoint-export-secrets.sh:/entrypoint-export-secrets.sh:ro
    secrets:
      - RABBITMQ_USER
      - RABBITMQ_PASSWORD
    ports:
      - "5672:5672"
      - "15672:15672"
    networks:
      ai-platform:
        ipv4_address: 172.20.0.41
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # PostgreSQL Database
  postgres:
    image: postgres:15
    container_name: ai-platform-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD
    entrypoint: ["/entrypoint-export-secrets.sh"]
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ../../sql/init:/docker-entrypoint-initdb.d:ro
      - ./docker/entrypoint-export-secrets.sh:/entrypoint-export-secrets.sh:ro
    secrets:
      - POSTGRES_PASSWORD
    ports:
      - "5432:5432"
    networks:
      ai-platform:
        ipv4_address: 172.20.0.42
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===========================================
  # MONITORING & MANAGEMENT
  # ===========================================
  
  # Health Monitor Service
  health-monitor:
    build:
      context: ../../scripts/platform-management
      dockerfile: Dockerfile.monitoring
    container_name: ai-platform-monitor
    restart: unless-stopped
    environment:
      - CHECK_INTERVAL=300
      - FAILURE_THRESHOLD=3
      - LOG_RETENTION_DAYS=7
    volumes:
      - platform_logs:/app/logs
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Docker access for monitoring
    networks:
      ai-platform:
        ipv4_address: 172.20.0.50
    depends_on:
      - chat-copilot-backend
      - autogen-studio
      - webhook-server
      - magentic-one
      - neo4j
      - genai-stack-api

  # ===========================================
  # NEO4J GENAI STACK SERVICES
  # ===========================================

  # Neo4j Database for GenAI Stack
  neo4j:
    image: neo4j:5.26
    container_name: ai-platform-neo4j
    restart: unless-stopped
    ports:
      - "7687:7687"   # Bolt protocol
      - "7474:7474"   # HTTP interface
    environment:
      - NEO4J_AUTH=${NEO4J_USERNAME:-neo4j}/${NEO4J_PASSWORD:-password}
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_db_tx__log_rotation_retention__policy=false
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_import:/var/lib/neo4j/import
    networks:
      ai-platform:
        ipv4_address: 172.20.0.60
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider localhost:7474 || exit 1"]
      interval: 15s
      timeout: 30s
      retries: 10

  # GenAI Stack Data Loader
  genai-stack-loader:
    build:
      context: ../../genai-stack
      dockerfile: loader.Dockerfile
    container_name: ai-platform-genai-loader
    restart: unless-stopped
    ports:
      - "8081:8080"   # Data import interface
      - "8502:8502"   # Streamlit interface
    environment:
      - NEO4J_URI=neo4j://neo4j:7687
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-password}
      - NEO4J_USERNAME=${NEO4J_USERNAME:-neo4j}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-sentence_transformer}
      - LANGCHAIN_ENDPOINT=${LANGCHAIN_ENDPOINT:-https://api.smith.langchain.com}
      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2:-false}
      - LANGCHAIN_PROJECT=${LANGCHAIN_PROJECT}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
    volumes:
      - genai_embedding_model:/embedding_model
    networks:
      ai-platform:
        ipv4_address: 172.20.0.61
    depends_on:
      neo4j:
        condition: service_healthy
    secrets:
      - OPENAI_API_KEY
      - GOOGLE_API_KEY
      - NEO4J_USERNAME
      - NEO4J_PASSWORD

  # GenAI Stack Support Bot
  genai-stack-bot:
    build:
      context: ../../genai-stack
      dockerfile: bot.Dockerfile
    container_name: ai-platform-genai-bot
    restart: unless-stopped
    ports:
      - "8501:8501"   # Support bot interface
    environment:
      - NEO4J_URI=neo4j://neo4j:7687
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-password}
      - NEO4J_USERNAME=${NEO4J_USERNAME:-neo4j}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      - LLM=${LLM:-llama2}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-sentence_transformer}
      - LANGCHAIN_ENDPOINT=${LANGCHAIN_ENDPOINT:-https://api.smith.langchain.com}
      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2:-false}
      - LANGCHAIN_PROJECT=${LANGCHAIN_PROJECT}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
    volumes:
      - genai_embedding_model:/embedding_model
    networks:
      ai-platform:
        ipv4_address: 172.20.0.62
    depends_on:
      neo4j:
        condition: service_healthy
    secrets:
      - OPENAI_API_KEY
      - GOOGLE_API_KEY
      - NEO4J_USERNAME
      - NEO4J_PASSWORD

  # GenAI Stack PDF Reader
  genai-stack-pdf:
    build:
      context: ../../genai-stack
      dockerfile: pdf_bot.Dockerfile
    container_name: ai-platform-genai-pdf
    restart: unless-stopped
    ports:
      - "8503:8503"   # PDF reader interface
    environment:
      - NEO4J_URI=neo4j://neo4j:7687
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-password}
      - NEO4J_USERNAME=${NEO4J_USERNAME:-neo4j}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      - LLM=${LLM:-llama2}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-sentence_transformer}
      - LANGCHAIN_ENDPOINT=${LANGCHAIN_ENDPOINT:-https://api.smith.langchain.com}
      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2:-false}
      - LANGCHAIN_PROJECT=${LANGCHAIN_PROJECT}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
    networks:
      ai-platform:
        ipv4_address: 172.20.0.63
    depends_on:
      neo4j:
        condition: service_healthy
    secrets:
      - OPENAI_API_KEY
      - GOOGLE_API_KEY
      - NEO4J_USERNAME
      - NEO4J_PASSWORD

  # GenAI Stack API Server
  genai-stack-api:
    build:
      context: ../../genai-stack
      dockerfile: api.Dockerfile
    container_name: ai-platform-genai-api
    restart: unless-stopped
    ports:
      - "8504:8504"   # API server
    environment:
      - NEO4J_URI=neo4j://neo4j:7687
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-password}
      - NEO4J_USERNAME=${NEO4J_USERNAME:-neo4j}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      - LLM=${LLM:-llama2}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-sentence_transformer}
      - LANGCHAIN_ENDPOINT=${LANGCHAIN_ENDPOINT:-https://api.smith.langchain.com}
      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2:-false}
      - LANGCHAIN_PROJECT=${LANGCHAIN_PROJECT}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
    volumes:
      - genai_embedding_model:/embedding_model
    networks:
      ai-platform:
        ipv4_address: 172.20.0.64
    depends_on:
      neo4j:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 http://localhost:8504/ || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 5
    secrets:
      - OPENAI_API_KEY
      - GOOGLE_API_KEY
      - NEO4J_USERNAME
      - NEO4J_PASSWORD

  # GenAI Stack Frontend
  genai-stack-frontend:
    build:
      context: ../../genai-stack
      dockerfile: front-end.Dockerfile
    container_name: ai-platform-genai-frontend
    restart: unless-stopped
    ports:
      - "8505:8505"   # Frontend interface
    networks:
      ai-platform:
        ipv4_address: 172.20.0.65
    depends_on:
      genai-stack-api:
        condition: service_healthy
secrets:
  # â”€â”€â”€â”€â”€ API / provider keys â”€â”€â”€â”€â”€
  AZURE_OPENAI_KEY:
    file: ../../secrets/AZURE_OPENAI_KEY
  OPENAI_API_KEY:
    file: ../../secrets/OPENAI_API_KEY
  GOOGLE_API_KEY:
    file: ../../secrets/GOOGLE_API_KEY
  OPENWEBUI_SECRET_KEY:
    file: ../../secrets/OPENWEBUI_SECRET_KEY
  LANGCHAIN_API_KEY:
    file: ../../secrets/LANGCHAIN_API_KEY
  ANTHROPIC_API_KEY:
    file: ../../secrets/ANTHROPIC_API_KEY
  GEMINI_API_KEY:
    file: ../../secrets/GEMINI_API_KEY

  # â”€â”€â”€â”€â”€ Database / queue credentials â”€â”€â”€â”€â”€
  RABBITMQ_USER:
    file: ../../secrets/RABBITMQ_USER
  RABBITMQ_PASSWORD:
    file: ../../secrets/RABBITMQ_PASSWORD
  POSTGRES_PASSWORD:
    file: ../../secrets/POSTGRES_PASSWORD

  # â”€â”€â”€â”€â”€ Optional GenAI / Neo4j â”€â”€â”€â”€â”€
  NEO4J_USERNAME:
    file: ../../secrets/NEO4J_USERNAME
  NEO4J_PASSWORD:
    file: ../../secrets/NEO4J_PASSWORD

  # â”€â”€â”€â”€â”€ Platform UI / misc â”€â”€â”€â”€â”€
  VSCODE_PASSWORD:
    file: ../../secrets/VSCODE_PASSWORD
  GITHUB_WEBHOOK_SECRET:
    file: ../../secrets/GITHUB_WEBHOOK_SECRET
  AWS_ACCESS_KEY_ID:
    file: ../../secrets/AWS_ACCESS_KEY_ID
  AWS_SECRET_ACCESS_KEY:
    file: ../../secrets/AWS_SECRET_ACCESS_KEY