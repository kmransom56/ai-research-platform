#!/bin/bash

# =============================================================================
# API Usage Monitoring Setup
# =============================================================================
# This script sets up monitoring for API usage across all services for the
# next 24-48 hours following credential rotation.
# =============================================================================

set -e

# Colors
GREEN='\033[0;32m'
RED='\033[0;31m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

print_status() { echo -e "${GREEN}[INFO]${NC} $1"; }
print_error() { echo -e "${RED}[ERROR]${NC} $1"; }
print_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
print_step() { echo -e "${BLUE}[STEP]${NC} $1"; }

echo
echo "üìä API USAGE MONITORING SETUP"
echo "==============================="
echo "Setting up 24-48 hour monitoring following credential rotation"
echo

# Create monitoring directory
mkdir -p logs/monitoring
mkdir -p scripts/monitoring/reports

# Step 1: Set up service usage tracking
print_step "1. Setting Up Service Usage Tracking"

cat > scripts/monitoring/track-api-usage.py << 'EOF'
#!/usr/bin/env python3
"""
API Usage Tracking Script
Monitors API usage across all services for security incident response
"""

import os
import json
import requests
import datetime
from pathlib import Path
import time

class APIUsageTracker:
    def __init__(self):
        self.start_time = datetime.datetime.now()
        self.log_dir = Path("logs/monitoring")
        self.log_dir.mkdir(exist_ok=True)
        
    def check_openai_usage(self):
        """Check OpenAI API usage"""
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            print("‚ö†Ô∏è  OpenAI API key not found in environment")
            return None
            
        try:
            # Note: OpenAI doesn't have a real-time usage API
            # This would need to be implemented with usage tracking in your app
            print("‚úÖ OpenAI API key is configured")
            return {"status": "configured", "service": "openai"}
        except Exception as e:
            print(f"‚ùå OpenAI API error: {e}")
            return None
    
    def check_anthropic_usage(self):
        """Check Anthropic API usage"""
        api_key = os.getenv('ANTHROPIC_API_KEY')
        if not api_key:
            print("‚ö†Ô∏è  Anthropic API key not found in environment")
            return None
            
        try:
            # Anthropic doesn't provide usage API, monitor via logs
            print("‚úÖ Anthropic API key is configured")
            return {"status": "configured", "service": "anthropic"}
        except Exception as e:
            print(f"‚ùå Anthropic API error: {e}")
            return None
    
    def check_github_usage(self):
        """Check GitHub API usage and rate limits"""
        token = os.getenv('GH_TOKEN')
        if not token:
            print("‚ö†Ô∏è  GitHub token not found in environment")
            return None
            
        try:
            headers = {
                'Authorization': f'token {token}',
                'Accept': 'application/vnd.github.v3+json'
            }
            
            response = requests.get('https://api.github.com/rate_limit', headers=headers)
            if response.status_code == 200:
                data = response.json()
                core_limit = data['resources']['core']
                print(f"‚úÖ GitHub API: {core_limit['used']}/{core_limit['limit']} requests used")
                return {
                    "service": "github",
                    "used": core_limit['used'],
                    "limit": core_limit['limit'],
                    "reset": core_limit['reset']
                }
            else:
                print(f"‚ùå GitHub API error: {response.status_code}")
                return None
        except Exception as e:
            print(f"‚ùå GitHub API error: {e}")
            return None
    
    def check_azure_openai_usage(self):
        """Check Azure OpenAI usage"""
        api_key = os.getenv('AZURE_OPENAI_KEY')
        endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')
        
        if not api_key or not endpoint:
            print("‚ö†Ô∏è  Azure OpenAI credentials not found")
            return None
            
        try:
            print("‚úÖ Azure OpenAI credentials configured")
            return {"status": "configured", "service": "azure_openai"}
        except Exception as e:
            print(f"‚ùå Azure OpenAI error: {e}")
            return None
    
    def generate_usage_report(self):
        """Generate comprehensive usage report"""
        print("\nüìä GENERATING API USAGE REPORT")
        print("=" * 40)
        
        report = {
            "timestamp": datetime.datetime.now().isoformat(),
            "monitoring_duration": str(datetime.datetime.now() - self.start_time),
            "services": {}
        }
        
        # Check each service
        services = [
            ("OpenAI", self.check_openai_usage),
            ("Anthropic", self.check_anthropic_usage),
            ("GitHub", self.check_github_usage),
            ("Azure OpenAI", self.check_azure_openai_usage)
        ]
        
        for service_name, check_func in services:
            print(f"\nChecking {service_name}...")
            result = check_func()
            if result:
                report["services"][service_name.lower().replace(" ", "_")] = result
        
        # Save report
        report_file = self.log_dir / f"usage_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"\nüìù Report saved to: {report_file}")
        return report

if __name__ == "__main__":
    tracker = APIUsageTracker()
    report = tracker.generate_usage_report()
    
    # Print summary
    print("\nüìã MONITORING SUMMARY")
    print("=" * 30)
    print(f"Services monitored: {len(report['services'])}")
    print(f"Report timestamp: {report['timestamp']}")
    
    # Check for any concerning usage
    if 'github' in report['services']:
        github_data = report['services']['github']
        usage_pct = (github_data['used'] / github_data['limit']) * 100
        if usage_pct > 80:
            print(f"‚ö†Ô∏è  GitHub API usage is high: {usage_pct:.1f}%")
        else:
            print(f"‚úÖ GitHub API usage normal: {usage_pct:.1f}%")
EOF

chmod +x scripts/monitoring/track-api-usage.py

print_status "‚úÖ API usage tracker created"

# Step 2: Set up automated monitoring schedule
print_step "2. Setting Up Monitoring Schedule"

cat > scripts/monitoring/monitor-daemon.sh << 'EOF'
#!/bin/bash

# =============================================================================
# Monitoring Daemon for 24-48 Hour Period
# =============================================================================

MONITORING_DURATION_HOURS=48
CHECK_INTERVAL_MINUTES=30
LOG_DIR="logs/monitoring"
ALERT_THRESHOLD_FILE="scripts/monitoring/alert_thresholds.json"

mkdir -p "$LOG_DIR"

# Create alert thresholds
cat > "$ALERT_THRESHOLD_FILE" << 'THRESHOLDS'
{
  "github_api_usage_percent": 85,
  "unusual_activity_requests_per_hour": 100,
  "failed_auth_attempts": 5,
  "new_ip_addresses": true
}
THRESHOLDS

echo "üîÑ Starting $MONITORING_DURATION_HOURS hour monitoring period..."
echo "üìä Check interval: $CHECK_INTERVAL_MINUTES minutes"
echo "üìù Logs: $LOG_DIR"
echo

START_TIME=$(date +%s)
END_TIME=$((START_TIME + MONITORING_DURATION_HOURS * 3600))

while [ $(date +%s) -lt $END_TIME ]; do
    CURRENT_TIME=$(date '+%Y-%m-%d %H:%M:%S')
    echo "[$CURRENT_TIME] Running API usage check..."
    
    # Run usage tracker
    python3 scripts/monitoring/track-api-usage.py
    
    # Log system status
    echo "[$CURRENT_TIME] System Status:" >> "$LOG_DIR/system_status.log"
    docker compose ps >> "$LOG_DIR/system_status.log" 2>/dev/null || echo "Docker not running" >> "$LOG_DIR/system_status.log"
    echo "---" >> "$LOG_DIR/system_status.log"
    
    # Wait for next check
    sleep $((CHECK_INTERVAL_MINUTES * 60))
done

echo "‚úÖ Monitoring period completed after $MONITORING_DURATION_HOURS hours"
EOF

chmod +x scripts/monitoring/monitor-daemon.sh

print_status "‚úÖ Monitoring daemon created"

# Step 3: Set up alert system
print_step "3. Setting Up Alert System"

cat > scripts/monitoring/check-alerts.py << 'EOF'
#!/usr/bin/env python3
"""
Alert System for API Monitoring
Checks for suspicious activity and sends notifications
"""

import json
import os
import datetime
from pathlib import Path
import glob

class AlertMonitor:
    def __init__(self):
        self.log_dir = Path("logs/monitoring")
        self.alert_log = self.log_dir / "alerts.log"
        
    def check_github_usage(self, report_data):
        """Check for GitHub API usage anomalies"""
        alerts = []
        
        if 'github' in report_data.get('services', {}):
            github_data = report_data['services']['github']
            usage_pct = (github_data['used'] / github_data['limit']) * 100
            
            if usage_pct > 85:
                alerts.append({
                    "severity": "HIGH",
                    "service": "GitHub",
                    "message": f"High API usage: {usage_pct:.1f}% ({github_data['used']}/{github_data['limit']})",
                    "timestamp": datetime.datetime.now().isoformat()
                })
        
        return alerts
    
    def check_for_unusual_patterns(self):
        """Check recent reports for unusual patterns"""
        alerts = []
        
        # Get recent reports (last 4 hours)
        recent_reports = []
        report_files = glob.glob(str(self.log_dir / "usage_report_*.json"))
        
        for report_file in sorted(report_files)[-8:]:  # Last 8 reports (4 hours if every 30 min)
            try:
                with open(report_file) as f:
                    recent_reports.append(json.load(f))
            except Exception as e:
                print(f"Warning: Could not read {report_file}: {e}")
        
        if len(recent_reports) < 2:
            return alerts
        
        # Check for increasing usage trends
        github_usage = []
        for report in recent_reports:
            if 'github' in report.get('services', {}):
                github_data = report['services']['github']
                usage_pct = (github_data['used'] / github_data['limit']) * 100
                github_usage.append(usage_pct)
        
        if len(github_usage) >= 3:
            # Check if usage is increasing rapidly
            recent_increase = github_usage[-1] - github_usage[-3]
            if recent_increase > 20:  # 20% increase in recent reports
                alerts.append({
                    "severity": "MEDIUM",
                    "service": "GitHub",
                    "message": f"Rapid usage increase detected: +{recent_increase:.1f}% in recent checks",
                    "timestamp": datetime.datetime.now().isoformat()
                })
        
        return alerts
    
    def log_alert(self, alert):
        """Log alert to file and print to console"""
        alert_line = f"[{alert['timestamp']}] {alert['severity']}: {alert['service']} - {alert['message']}\n"
        
        with open(self.alert_log, 'a') as f:
            f.write(alert_line)
        
        # Color-coded console output
        color = '\033[0;31m' if alert['severity'] == 'HIGH' else '\033[1;33m'  # Red for HIGH, Yellow for MEDIUM
        print(f"{color}üö® ALERT: {alert['service']} - {alert['message']}\033[0m")
    
    def run_alert_check(self):
        """Run comprehensive alert check"""
        print("üîç Running alert check...")
        
        # Get latest usage report
        report_files = glob.glob(str(self.log_dir / "usage_report_*.json"))
        if not report_files:
            print("No usage reports found")
            return
        
        latest_report_file = max(report_files)
        try:
            with open(latest_report_file) as f:
                latest_report = json.load(f)
        except Exception as e:
            print(f"Error reading latest report: {e}")
            return
        
        # Check various alert conditions
        all_alerts = []
        all_alerts.extend(self.check_github_usage(latest_report))
        all_alerts.extend(self.check_for_unusual_patterns())
        
        # Process alerts
        if all_alerts:
            print(f"‚ö†Ô∏è  Found {len(all_alerts)} alerts")
            for alert in all_alerts:
                self.log_alert(alert)
        else:
            print("‚úÖ No alerts detected")
        
        return all_alerts

if __name__ == "__main__":
    monitor = AlertMonitor()
    alerts = monitor.run_alert_check()
    
    if alerts:
        print(f"\nüì¢ ALERT SUMMARY: {len(alerts)} alerts generated")
        print("Check logs/monitoring/alerts.log for full details")
    else:
        print("\n‚úÖ All systems normal")
EOF

chmod +x scripts/monitoring/check-alerts.py

print_status "‚úÖ Alert system created"

# Step 4: Create monitoring dashboard
print_step "4. Creating Monitoring Dashboard"

cat > scripts/monitoring/dashboard.py << 'EOF'
#!/usr/bin/env python3
"""
Simple monitoring dashboard for API usage
"""

import json
import glob
from pathlib import Path
import datetime

class MonitoringDashboard:
    def __init__(self):
        self.log_dir = Path("logs/monitoring")
    
    def generate_dashboard(self):
        """Generate text-based dashboard"""
        print("üìä API MONITORING DASHBOARD")
        print("=" * 50)
        print(f"Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        # Get latest report
        report_files = glob.glob(str(self.log_dir / "usage_report_*.json"))
        if not report_files:
            print("‚ùå No usage reports found")
            return
        
        latest_report_file = max(report_files)
        try:
            with open(latest_report_file) as f:
                latest_report = json.load(f)
        except Exception as e:
            print(f"‚ùå Error reading report: {e}")
            return
        
        # Display service status
        print("üîê SERVICE STATUS:")
        services = latest_report.get('services', {})
        
        for service_name, service_data in services.items():
            status_icon = "‚úÖ" if service_data.get('status') == 'configured' else "üìä"
            print(f"  {status_icon} {service_name.replace('_', ' ').title()}")
            
            if service_name == 'github' and 'used' in service_data:
                usage_pct = (service_data['used'] / service_data['limit']) * 100
                bar_length = 20
                filled_length = int(bar_length * usage_pct / 100)
                bar = '‚ñà' * filled_length + '‚ñë' * (bar_length - filled_length)
                
                print(f"    Usage: {usage_pct:.1f}% [{bar}] {service_data['used']}/{service_data['limit']}")
                
                # Reset time
                reset_time = datetime.datetime.fromtimestamp(service_data['reset'])
                print(f"    Resets: {reset_time.strftime('%H:%M:%S')}")
        
        print()
        
        # Show recent activity
        print("üìà RECENT ACTIVITY:")
        recent_files = sorted(glob.glob(str(self.log_dir / "usage_report_*.json")))[-5:]
        
        for report_file in recent_files:
            try:
                with open(report_file) as f:
                    report = json.load(f)
                
                timestamp = datetime.datetime.fromisoformat(report['timestamp'])
                service_count = len(report.get('services', {}))
                print(f"  {timestamp.strftime('%H:%M:%S')} - {service_count} services checked")
                
            except Exception as e:
                print(f"  ‚ùå Error reading {Path(report_file).name}")
        
        print()
        
        # Show alerts
        alert_file = self.log_dir / "alerts.log"
        if alert_file.exists():
            print("üö® RECENT ALERTS:")
            try:
                with open(alert_file) as f:
                    alert_lines = f.readlines()[-5:]  # Last 5 alerts
                
                if alert_lines:
                    for line in alert_lines:
                        print(f"  {line.strip()}")
                else:
                    print("  ‚úÖ No recent alerts")
            except Exception:
                print("  ‚ùå Error reading alerts")
        else:
            print("üö® ALERTS: No alert log found")
        
        print()
        print("üîó MONITORING LINKS:")
        print("  ‚Ä¢ OpenAI Usage: https://platform.openai.com/usage")
        print("  ‚Ä¢ Azure Costs: https://portal.azure.com/#view/Microsoft_Azure_CostManagement")
        print("  ‚Ä¢ GitHub Actions: Repository ‚Üí Settings ‚Üí Secrets and variables")
        print()

if __name__ == "__main__":
    dashboard = MonitoringDashboard()
    dashboard.generate_dashboard()
EOF

chmod +x scripts/monitoring/dashboard.py

print_status "‚úÖ Monitoring dashboard created"

# Step 5: Create startup script
print_step "5. Creating Monitoring Startup"

cat > scripts/monitoring/start-monitoring.sh << 'EOF'
#!/bin/bash

# =============================================================================
# Start 24-48 Hour API Monitoring
# =============================================================================

echo "üöÄ STARTING API USAGE MONITORING"
echo "================================="
echo

# Create logs directory
mkdir -p logs/monitoring

# Initial baseline check
echo "üìä Running initial API usage baseline check..."
python3 scripts/monitoring/track-api-usage.py

# Show dashboard
echo
python3 scripts/monitoring/dashboard.py

echo
echo "üîÑ Starting background monitoring daemon..."
echo "This will run for 48 hours, checking every 30 minutes"
echo

# Start monitoring in background
nohup bash scripts/monitoring/monitor-daemon.sh > logs/monitoring/daemon.log 2>&1 &
DAEMON_PID=$!

echo "‚úÖ Monitoring daemon started (PID: $DAEMON_PID)"
echo "üìù Daemon log: logs/monitoring/daemon.log"
echo
echo "üìã MONITORING COMMANDS:"
echo "‚Ä¢ View dashboard:    python3 scripts/monitoring/dashboard.py"
echo "‚Ä¢ Check alerts:      python3 scripts/monitoring/check-alerts.py" 
echo "‚Ä¢ Stop monitoring:   pkill -f monitor-daemon.sh"
echo "‚Ä¢ View logs:         tail -f logs/monitoring/daemon.log"
echo
echo "üö® IMPORTANT: Monitor these dashboards manually as well:"
echo "‚Ä¢ OpenAI Usage: https://platform.openai.com/usage"
echo "‚Ä¢ Azure Costs: https://portal.azure.com/#view/Microsoft_Azure_CostManagement"
echo "‚Ä¢ GitHub API Limits: https://github.com/settings/personal-access-tokens"
echo

echo "PID:$DAEMON_PID" > logs/monitoring/daemon.pid
echo "‚è∞ Monitoring will run for 48 hours. Check back periodically!"
EOF

chmod +x scripts/monitoring/start-monitoring.sh

print_status "‚úÖ Monitoring startup script created"

echo

# Step 6: Run secret scanner as requested
print_step "6. Running Secret Scanner Verification"

if [ -f "scripts/security/secret-scanner.py" ]; then
    print_status "Running comprehensive secret scanner..."
    python3 scripts/security/secret-scanner.py || print_warning "Scanner completed with warnings (this is normal)"
else
    print_error "Secret scanner not found"
fi

echo

# Summary and next steps
print_step "üìã MONITORING SETUP COMPLETE"

echo "‚úÖ 24-48 Hour API Monitoring System Ready"
echo
echo "üöÄ TO START MONITORING:"
echo "   ./scripts/monitoring/start-monitoring.sh"
echo
echo "üìä MONITORING FEATURES:"
echo "‚Ä¢ Automated usage tracking every 30 minutes"
echo "‚Ä¢ Real-time alert system for unusual activity"
echo "‚Ä¢ Dashboard for quick status overview"
echo "‚Ä¢ 48-hour continuous monitoring period"
echo
echo "üîó MANUAL MONITORING REQUIRED:"
echo "‚Ä¢ OpenAI Usage Dashboard: https://platform.openai.com/usage"
echo "‚Ä¢ Azure Cost Management: https://portal.azure.com/#view/Microsoft_Azure_CostManagement"
echo "‚Ä¢ GitHub API Rate Limits: Check repository settings"
echo
echo "‚ö†Ô∏è  NEXT STEPS FOR COMPLETE SECURITY:"
echo "1. Set up billing alerts in each service provider dashboard"
echo "2. Enable security notifications in service accounts"
echo "3. Schedule 90-day key rotation reminders"
echo "4. Review monitoring logs daily for the next 48 hours"
echo

print_status "Run './scripts/monitoring/start-monitoring.sh' to begin monitoring now!"